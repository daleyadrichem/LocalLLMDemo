{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experimenting with prompts and settings\n",
        "\n",
        "This notebook lets you play with temperature, max tokens, and different prompts using the\n",
        "`LocalLLM.generate` method.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llm_local.llm_client import LocalLLM, LocalLLMConfig\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize the model\n",
        "\n",
        "Make sure Ollama is running and a model is available (e.g. `llama3.2:3b`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = LocalLLMConfig(model=\"llama3.2:3b\")\n",
        "llm = LocalLLM(config=config)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Try a basic explanation prompt\n",
        "\n",
        "Here we ask the model to explain a concept in different ways.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"Explain what a neural network is to a high school student.\"\n",
        "response = llm.generate(prompt, temperature=0.2)\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compare different temperatures\n",
        "\n",
        "Higher temperature â†’ more diverse / creative answers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for temp in [0.0, 0.3, 0.7]:\n",
        "    print(f\"\\n=== temperature={temp} ===\\n\")\n",
        "    response = llm.generate(prompt, temperature=temp)\n",
        "    print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use a system prompt\n",
        "\n",
        "We can define a role for the assistant using the `system_prompt`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "system_prompt = (\n",
        "    \"You are an expert AI tutor. You always answer with short, clear explanations \"\n",
        "    \"and concrete examples.\"\n",
        ")\n",
        "response = llm.generate(prompt, system_prompt=system_prompt, temperature=0.2)\n",
        "print(response)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
