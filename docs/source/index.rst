Welcome to llm-local's documentation
====================================

llm-local is a small, reusable library for interacting with a local LLM backend (e.g. Ollama),
plus higher-level developer services built on top of that runtime client.

.. toctree::
   :maxdepth: 2
   :caption: Contents

   getting_started
   usage
   api/index
